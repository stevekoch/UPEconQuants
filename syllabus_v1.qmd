---
title: "EKT 813: Quants"
subtitle: "Syllabus"
author: "Prof SF Koch"
date: "today"
format: 
  html:
    theme: cosmo
    toc: true
editor: visual
bibliography: module.bib
---

# The Module

## Meeting time

The module timeframe was determined by the department. Please, if you would like to arrange an appointment, contact me via e-mail. In general, I will not be available the first half of the semester on a Thursday. I will also not be available Friday mornings, as I use that time to meet with the PG students that I supervise.

-   Time: Wednesday, 1630-1900
-   Where: Tukkiewerf 1-37
-   Instructor: Prof SF Koch
-   Office: Tukkiewerf 2-05
-   Office Hours:
    -   Wednesdays, 1430-1630
    -   By appointment
-   Contact: <a href="mailto:steve.koch@up.ac.za">steve.koch\@up.ac.za</a>
- [GitHub](https://github.com/stevekoch/UPEconQuants)

## Module Description

This is the syllabus for EKT 813, which is a quants class. It is also required for all M students in the department. 
Quants generally covers some mathematical economics, some probability and some fairly standard regression ideas. My version is concerned with repeatability, specifically using software as a tool to make your student work-life easier.  Given the advances in AI, one might be tempted to believe that AI will do all the software work. It may; despite that, economists still need to be able to think logically and critically. Thus, the old adage remains relevant: **garbage in, garbage out**. The reality of any module of this sort is that the depth and breadth of material that can be incorporated is unlimited, but our own resources for digesting and learning is limited... Yes, another economics conundrum. 

My plan is to have three basic topics: (i) Data, (ii) Unconditional outcomes and (iii) Conditional outcomes. In terms of data, we will spend three or four weeks accessing publicly available South African data, putting it into shape for analytical purposes, describing it and illustrating it. In terms of unconditional outcomes, we will spend another three or four weeks. A fair share of this component will be building on illustrations, to examine histograms, densities, distributions, and such. In addition, we will discuss probabilty and estimation related to them. Such information is often used to examine stochastic dominance and inequality. In the final section of the module, we will spend 6-7 weeks examining conditional outcomes, which of course, includes regression and testing. I hope to add to that to include conditional quantiles and conditional nonparametric estimates, as well as decompositing and influence functions. 

Your grade in the module will be determined by four *projects*, one in each of the first two sections and two in the final section. The point of each project is to get you to apply what has been developed. Each project will be weighted equally; please, note that late submission will be penalized.

Finally, it is expected that you are comfortable with standard statistics concepts up to an advanced undergraduate level. Thus, if you struggle with @godwin2022quantitative, or something similar, you will find this module to be rather complicated. The text also includes some basic R coding that you might find helpful.


## Course Objectives

In principle, there are plenty of topics to consider. With technology and lots of bright people working on lots of different things, there will be even more topics to consider next week! However, we have to draw a line and try to present something that M students can use. Thus, as noted above, I have limited the focus to data manipulation and presentation, some important distributional ideas, and, finally, conditional outcomes. Along the way, we will certainly see math and probability.

One of the most important aspects of this module is the underlying value of **repeatability**. Hopefully, this term is obvious. If not, just note that it relates back to the point of the scientific method, where we (i) ask a question, (ii) learn what is already known, (iii) develop a hypothesis, (iv) test it with a *repeatable* structured test to gather evidence, (v) analyse the data, and (vi) communicate findings. This objective will be the main focus of our initial section of the module. 

The majority of research in economics is empirically focused. It is underscored by data, a large share of which is produced by statistical agencies. One key to repeatability in the use of such data is absolute clarity - nowadays that includes structured code development (and reasons for decisions made along the way) and instructions to others - such that *your* experiment (data analysis) can be repeated and yeild the same results.  

Another important aspect of this module is that there are a range of different data types - we will only look at a subset of such types - and those differences should lead us to present it according to type. Related to that is the simple process of data exploration. No amount of technical modelling will *fix* a poor understanding of the data at hand. Thus, it is necessary to understand distributional aspects and potential nonlinearities in relationships. This objective will be the main focus of our second section of the module.

Much of econometrics is focused on causal evidence - more than a few recent Nobel prizes in the field have been awarded for advances in causality. Getting to that point, however, is generally quite problematic. For example, instruments are rare, as are truly exogenous natural experiments. Despite that, one can often learn plenty about various facets of the economy by undertaking detailed descriptive analysis. Thus, the final objective of the module is to develop an understanding of a set of tools, methods and tests to develop a deeper understanding of the performance of various parts of the economy. That is our focus in section three of the module, although it will also relate to section two, and cannot be reasonably accomplished if we do not achieve our first section's objective.

# Outline

## Topic 1 - Data

The first section of the module will cover three to four weeks: Feb 11, 18, 25 & March 4(?). We will focus on accessing pulblicly available data. We will make use of [DataFirst](https://www.datafirst.uct.ac.za), which is housed at the University of Cape Town. It is a large repository of data, as well as a group of researchers dedicated to data quality. Researchers there have fixed some SA data, uncovered problems with other SA data, so we at least know about them, and developed data series' based on surveys that are oft-repeated. There are numerous additional repositories in the world, but they are out of scope for this module.

I work in R, and, therefore, nearly everything I will cover will operate in R. One of the reasons I work in R is because I care about repeatability, which R and associated options make easy; many of the same options are also availalble for Stan and Python (even Stata, although not quite as seemlessly).^[It should be noted that Python has a rather similar structure to R, and numerous researchers feel that PANDAS is an excellent panel data component. It is a python package.] In other words, there are many other software languages out there; for the most part, they all pretty much do the same thing, although some programs do some things better than others... 

Another reason I use R is because there are no costs involved in its purchase. It is also widely appreciated by the Machine Learning community, many even suggesting it is faster and more useful than python (but opinions vary on this). It is exceptionally flexible, can be incorporated directly into documents (such as reports and research manuscripts) and there is an army of online support for code help. 

There are numerous advances (openAI, for example) out there that can help you program in R or most any other language, as long as you understand the basic workings. Thus, I will not devote my time to "teaching" everything you might want to know about R or any other language - see above comment regarding economics conundrums. 

The exceptional online text by @wickham2023r4ds is amazing for developing an understanding of how R can be used to interact with all types of data; @imai2022quantitative is a bit more limited, but offers some useful guidance in manipulating data to understand it better. Unfortunately, neither directly offers help for interacting with the data we will use in this module. Thus, I will present some code that is specific to that data. I am not a coding genious, so it is also not going to be as 'pretty' as their code. 

For the most part, we will rely on something called the ```tidyverse``` [@tidyverse], which is a slighltly more human way of interacting with data. Our main goal is to read in data from different sources, mutate it, think about issues related to missingness (including its extent and potential selectivity), merge data (when necessary) and general cleaning and wrangling. The tidyverse can also be used to develop tables, as can other packages, and illustrations of different types of data. 

Each of the following online texts, websites and additional resources are also likely to help with R code for various features under topic 1: @soetewey2023dplyr; @machut_cornwell_dataanalytics; @datacamp_dplyr_cheatsheet; @mesquita2021thinking; @heiss2016using; and @wilke2019fundamentals.


## Topic 2 - Unconditional outcomes

The second section of the module is also expected to cover three to four weeks: March 4(?), 11, 18, 25 and April 8(?). This section contains some probability and statistics, as it extends the data development section to describe a range of features of both univariate and multivariate data. Along with this, we will examine stochastic dominance and inequality and other more general 'tests' and 'comparisons' of discrete and continuous data. 

Our discussion will be based on the statistics sections of various econometrics texts, such as @camerontrivedi; @cunningham2021causal; @godwin2022quantitative; @hansen2022econometrics; and @wooldridge, amongst others. Additionally, we will incorporate nonparametric and similar texts/references, such as @DiNardoTobias; @racine2008nonparametric; @racine_entropy_np and even @james2021introduction. Finally, we will likely benefit from general data analytic, inequalaty and stochastic dominance discussions, such as those in @deaton1997analysis; @odonnell2008health; and @TOHIDI2026102461.


## Topic 3 - Conditional outcomes

The final section of the module is scheduled to cover six to seven weeks or more: April 8(?), 15, 22, 29, May 6, 13, 20 and 27, depending on need and availability.^[Currenlty, April 8 is unlikely, given my current travel schedule.] The focus of the final section is conditionality; thus, we will revisit the preceding discussion, extending it to capture outcomes conditional on additional controls. Although the standard framework for this is regression, which we will cover, we will extend this to capture quantiles, nonparametric regression (I hope), address a variety of economic interpretations, such as elasticities, which might require marginal effects analysis. Related to that, we are likely to concerned with testing and standard error issues, which means the consideration of the bootstrap. An important application that arises here is based on decomposition. Essentially, the goal is to develop a counterfactual that attempts to capture/force underlying similarities in the conditioning variables. Such analysis can depend on the nonparametric densities discussed in the previous section, although other options are also available, such as that related to influence functions.

In addition to many of the preceding resource texts, it is expected that the following will be of use: @angristpischke; @fortin2011decomposition; @FirpoFortinLemieux2018; @gelman2007data; @heiss2016using; @HesterbergBoot; @kleiber2008applied; @johnston1997econometric; @kennedy2008guide; @koenker2005quantile; @koenker_quantreg_vignette; @racine2019reproducible; @verbeek2017guide. 

# Grading 

This is designed to be fairly simple. There are four 'projects' through the term. Each will require some sort of clear 'readme' file that outlines exact steps to follow, the decisions made to undertake the analysis, and so on.   Furthermore, each will be underpinned by a report that properly references software, data, relevant packages (if needed). 

Each of the projects is 25% of the final mark. Grading occurs across the following components:

  - Ease of replication, which is underscored by the quality of the comments and related documentation
  - Correctness of the completion of the activities and
  - Clarity and reasonableness of the discussion contained within the 'report' document.

The projects will build on each other, beginning with accessing data - students will generally be asked to make use of different data sources. Students will then need to present the data through tables and descriptive illustrations. From there, the data will be used for different types of analysis related to the various topics discussed in the module.

Of course, the report should describe the project, some relevant statistics or modelling 'theory', present and describe the results, inclusive of cross-references and notes for tables and figures.m I do hope to keep the writing component down to 4-5 pages, although tables and figures are likely to make this longer.

Assignments will have a submission deadline. I presume they will be emailed, although I might include upload via click-up. Late projects are reduced 10% for each day late (up to 6 days). No project will be accepted a week late or more.

# References
